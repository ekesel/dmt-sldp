---
phase: 10
plan: 2
wave: 1
---

# Plan 10.2: Aggregation Engine Implementation

## Objective
Implement high-performance background tasks to compute and persist daily and sprint metrics.

## Context
- .gsd/phases/10/RESEARCH.md
- backend/data/tasks.py
- backend/core/celery.py

## Tasks

<task type="auto">
  <name>Implement Aggregation Logic</name>
  <files>backend/data/tasks.py</files>
  <action>
    Implement `aggregate_tenant_metrics(tenant_id)` task.
    - Switch to tenant schema.
    - Compute metrics for the previous day (T-1): total items, compliant count, cycles times, PR counts.
    - Upsert results into `DailyMetric`.
    - Implement `run_daily_aggregation` to iterate over all tenants and dispatch `aggregate_tenant_metrics`.
  </action>
  <verify>docker-compose run --rm backend python3 manage.py tenant_command shell --schema=test_tenant -c "from data.tasks import aggregate_tenant_metrics; aggregate_tenant_metrics.delay(1)"</verify>
  <done>Aggregation tasks implemented and manually verified for a single tenant.</done>
</task>

<task type="auto">
  <name>Configure Celery Beat Schedule</name>
  <files>backend/core/celery.py</files>
  <action>
    Add `run_daily_aggregation` to the `CELERY_BEAT_SCHEDULE`.
    - Set to run daily at 00:00 UTC.
  </action>
  <verify>grep "run_daily_aggregation" backend/core/celery.py</verify>
  <done>Beat schedule updated to include daily metric aggregation.</done>
</task>

## Success Criteria
- [ ] `run_daily_aggregation` task exists and correctly iterates over tenants.
- [ ] `aggregate_tenant_metrics` correctly computes and persists data for the target date.
- [ ] Celery Beat is configured to trigger the workflow daily.
